{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OilZGlwQl-bF"
   },
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BnczALpul-bG"
   },
   "outputs": [],
   "source": [
    "#  Load the dataset\n",
    "data = pd.read_csv(r\"D:\\kaggle\\train_dataset.csv\", index_col=0)\n",
    "eval = pd.read_csv(r\"D:\\kaggle\\eval_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDFYfQKQl-bH",
    "outputId": "2acc9301-f4f9-420b-c323-c5d1295339f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  Gender  Reviews_Read    Price  Discount  Category  Items_In_Cart  \\\n",
      "id                                                                           \n",
      "0   20.0     1.0           1.0  623.797      25.0       4.0            4.0   \n",
      "1   22.0     0.0           5.0  549.324      50.0       4.0            8.0   \n",
      "2   18.0     0.0           2.0  489.191      19.0       0.0            5.0   \n",
      "3   25.0     0.0           3.0  769.450      16.0       4.0            5.0   \n",
      "4   22.0     1.0           2.0   39.308      35.0       0.0            3.0   \n",
      "\n",
      "   Time_of_Day  Email_Interaction Device_Type Payment_Method Referral_Source  \\\n",
      "id                                                                             \n",
      "0    afternoon                0.0     Desktop         PayPal          Direct   \n",
      "1      evening                0.0     Desktop           Bank   Search_engine   \n",
      "2    afternoon                0.0      Mobile           Bank           Email   \n",
      "3      evening                0.0      Mobile         Credit    Social_media   \n",
      "4      evening                0.0      Mobile            NaN           Email   \n",
      "\n",
      "    Socioeconomic_Status_Score  Engagement_Score  Purchase  \n",
      "id                                                          \n",
      "0                         9.13          2.167873         0  \n",
      "1                         9.53          3.494243         0  \n",
      "2                         0.70          2.093568         0  \n",
      "3                         6.83          3.644859         0  \n",
      "4                         1.86          4.171256         0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6500 entries, 0 to 6499\n",
      "Data columns (total 15 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Age                         5530 non-null   float64\n",
      " 1   Gender                      6359 non-null   float64\n",
      " 2   Reviews_Read                6373 non-null   float64\n",
      " 3   Price                       6355 non-null   float64\n",
      " 4   Discount                    6376 non-null   float64\n",
      " 5   Category                    6366 non-null   float64\n",
      " 6   Items_In_Cart               6359 non-null   float64\n",
      " 7   Time_of_Day                 6371 non-null   object \n",
      " 8   Email_Interaction           6373 non-null   float64\n",
      " 9   Device_Type                 6376 non-null   object \n",
      " 10  Payment_Method              5491 non-null   object \n",
      " 11  Referral_Source             5503 non-null   object \n",
      " 12  Socioeconomic_Status_Score  6380 non-null   float64\n",
      " 13  Engagement_Score            6365 non-null   float64\n",
      " 14  Purchase                    6500 non-null   int64  \n",
      "dtypes: float64(10), int64(1), object(4)\n",
      "memory usage: 812.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Basic Data Overview\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNazAdxjl-bH",
    "outputId": "69a5c0f0-bf24-4d32-8107-2aae4c15c753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Age                            970\n",
      "Gender                         141\n",
      "Reviews_Read                   127\n",
      "Price                          145\n",
      "Discount                       124\n",
      "Category                       134\n",
      "Items_In_Cart                  141\n",
      "Time_of_Day                    129\n",
      "Email_Interaction              127\n",
      "Device_Type                    124\n",
      "Payment_Method                1009\n",
      "Referral_Source                997\n",
      "Socioeconomic_Status_Score     120\n",
      "Engagement_Score               135\n",
      "Purchase                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values (if any)\n",
    "# Checking for missing values\n",
    "print(\"Missing values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt8BtPiiUEfR",
    "outputId": "433c747e-8819-4cce-d349-952292861a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Age                           530\n",
      "Gender                         59\n",
      "Reviews_Read                   73\n",
      "Price                          55\n",
      "Discount                       76\n",
      "Category                       66\n",
      "Items_In_Cart                  59\n",
      "Time_of_Day                    71\n",
      "Email_Interaction              73\n",
      "Device_Type                    76\n",
      "Payment_Method                491\n",
      "Referral_Source               503\n",
      "Socioeconomic_Status_Score     80\n",
      "Engagement_Score               65\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for eval_dataset missing values\n",
    "print(\"Missing values:\\n\", eval.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTevfHTxT8iW",
    "outputId": "818d69ad-d772-46cb-ced6-60dfd45627dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate data\n",
    "print(data.duplicated().sum())\n",
    "print(eval.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in categorical variables (before replacement):\n",
      "Gender               0\n",
      "Category             0\n",
      "Time_of_Day          0\n",
      "Email_Interaction    0\n",
      "Device_Type          0\n",
      "Payment_Method       0\n",
      "Referral_Source      0\n",
      "dtype: int64\n",
      "Missing values in categorical variables (after replacement):\n",
      "Gender                141\n",
      "Category              134\n",
      "Time_of_Day           129\n",
      "Email_Interaction     127\n",
      "Device_Type           124\n",
      "Payment_Method       1009\n",
      "Referral_Source       997\n",
      "dtype: int64\n",
      "Missing values in categorical variables (before replacement):\n",
      "Gender               0\n",
      "Category             0\n",
      "Time_of_Day          0\n",
      "Email_Interaction    0\n",
      "Device_Type          0\n",
      "Payment_Method       0\n",
      "Referral_Source      0\n",
      "dtype: int64\n",
      "Missing values in categorical variables (after replacement):\n",
      "Gender                59\n",
      "Category              66\n",
      "Time_of_Day           71\n",
      "Email_Interaction     73\n",
      "Device_Type           76\n",
      "Payment_Method       491\n",
      "Referral_Source      503\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Gender', 'Category', 'Time_of_Day', 'Email_Interaction', 'Device_Type', 'Payment_Method', 'Referral_Source']\n",
    "numerical_cols = ['Age', 'Reviews_Read', 'Price', 'Discount', 'Items_In_Cart', 'Socioeconomic_Status_Score', 'Engagement_Score']\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(dataset, categorical_cols, numerical_cols):\n",
    "    # 1. Handle categorical variables\n",
    "    # Identify original string columns\n",
    "    original_string_cols = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Identify numerical columns that were converted to strings\n",
    "    converted_string_cols = list(set(categorical_cols) - set(original_string_cols))\n",
    "\n",
    "    # Standardize categorical variables\n",
    "    for col in original_string_cols:\n",
    "        dataset[col] = dataset[col].astype(str).str.lower().str.replace('0', 'o')\n",
    "\n",
    "    for col in converted_string_cols:\n",
    "        dataset[col] = dataset[col].astype(str)\n",
    "\n",
    "    # Check and print the count of missing values in categorical variables (before replacement)\n",
    "    print(f\"Missing values in categorical variables (before replacement):\\n{dataset[categorical_cols].isna().sum()}\")\n",
    "\n",
    "    # Replace implicit missing values with explicit pd.NA\n",
    "    dataset[categorical_cols] = dataset[categorical_cols].replace(['', 'nan', 'N/A', '<NA>', 'None'], pd.NA)\n",
    "\n",
    "    # Check and print the count of missing values in categorical variables (after replacement)\n",
    "    print(f\"Missing values in categorical variables (after replacement):\\n{dataset[categorical_cols].isna().sum()}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Clean the two datasets\n",
    "data = preprocess_data(data, categorical_cols, numerical_cols)\n",
    "eval = preprocess_data(eval, categorical_cols, numerical_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6500 entries, 0 to 6499\n",
      "Data columns (total 34 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Age                            6500 non-null   float64\n",
      " 1   Reviews_Read                   6500 non-null   float64\n",
      " 2   Price                          6500 non-null   float64\n",
      " 3   Discount                       6500 non-null   float64\n",
      " 4   Items_In_Cart                  6500 non-null   float64\n",
      " 5   Socioeconomic_Status_Score     6500 non-null   float64\n",
      " 6   Engagement_Score               6500 non-null   float64\n",
      " 7   Purchase                       6500 non-null   int64  \n",
      " 8   Gender_0.0                     6500 non-null   bool   \n",
      " 9   Gender_1.0                     6500 non-null   bool   \n",
      " 10  Category_0.0                   6500 non-null   bool   \n",
      " 11  Category_1.0                   6500 non-null   bool   \n",
      " 12  Category_2.0                   6500 non-null   bool   \n",
      " 13  Category_3.0                   6500 non-null   bool   \n",
      " 14  Category_4.0                   6500 non-null   bool   \n",
      " 15  Time_of_Day_afternoon          6500 non-null   bool   \n",
      " 16  Time_of_Day_evening            6500 non-null   bool   \n",
      " 17  Time_of_Day_morning            6500 non-null   bool   \n",
      " 18  Email_Interaction_0.0          6500 non-null   bool   \n",
      " 19  Email_Interaction_1.0          6500 non-null   bool   \n",
      " 20  Device_Type_desktop            6500 non-null   bool   \n",
      " 21  Device_Type_mobile             6500 non-null   bool   \n",
      " 22  Device_Type_tablet             6500 non-null   bool   \n",
      " 23  Payment_Method_bank            6500 non-null   bool   \n",
      " 24  Payment_Method_cash            6500 non-null   bool   \n",
      " 25  Payment_Method_credit          6500 non-null   bool   \n",
      " 26  Payment_Method_paypal          6500 non-null   bool   \n",
      " 27  Payment_Method_unknown         6500 non-null   bool   \n",
      " 28  Referral_Source_ads            6500 non-null   bool   \n",
      " 29  Referral_Source_direct         6500 non-null   bool   \n",
      " 30  Referral_Source_email          6500 non-null   bool   \n",
      " 31  Referral_Source_search_engine  6500 non-null   bool   \n",
      " 32  Referral_Source_social_media   6500 non-null   bool   \n",
      " 33  Referral_Source_unknown        6500 non-null   bool   \n",
      "dtypes: bool(26), float64(7), int64(1)\n",
      "memory usage: 622.1 KB\n",
      "Structure of the training dataset:\n",
      " None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3500 entries, 6500 to 9999\n",
      "Data columns (total 33 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Age                            3500 non-null   float64\n",
      " 1   Reviews_Read                   3500 non-null   float64\n",
      " 2   Price                          3500 non-null   float64\n",
      " 3   Discount                       3500 non-null   float64\n",
      " 4   Items_In_Cart                  3500 non-null   float64\n",
      " 5   Socioeconomic_Status_Score     3500 non-null   float64\n",
      " 6   Engagement_Score               3500 non-null   float64\n",
      " 7   Gender_0.0                     3500 non-null   bool   \n",
      " 8   Gender_1.0                     3500 non-null   bool   \n",
      " 9   Category_0.0                   3500 non-null   bool   \n",
      " 10  Category_1.0                   3500 non-null   bool   \n",
      " 11  Category_2.0                   3500 non-null   bool   \n",
      " 12  Category_3.0                   3500 non-null   bool   \n",
      " 13  Category_4.0                   3500 non-null   bool   \n",
      " 14  Time_of_Day_afternoon          3500 non-null   bool   \n",
      " 15  Time_of_Day_evening            3500 non-null   bool   \n",
      " 16  Time_of_Day_morning            3500 non-null   bool   \n",
      " 17  Email_Interaction_0.0          3500 non-null   bool   \n",
      " 18  Email_Interaction_1.0          3500 non-null   bool   \n",
      " 19  Device_Type_desktop            3500 non-null   bool   \n",
      " 20  Device_Type_mobile             3500 non-null   bool   \n",
      " 21  Device_Type_tablet             3500 non-null   bool   \n",
      " 22  Payment_Method_bank            3500 non-null   bool   \n",
      " 23  Payment_Method_cash            3500 non-null   bool   \n",
      " 24  Payment_Method_credit          3500 non-null   bool   \n",
      " 25  Payment_Method_paypal          3500 non-null   bool   \n",
      " 26  Payment_Method_unknown         3500 non-null   bool   \n",
      " 27  Referral_Source_ads            3500 non-null   bool   \n",
      " 28  Referral_Source_direct         3500 non-null   bool   \n",
      " 29  Referral_Source_email          3500 non-null   bool   \n",
      " 30  Referral_Source_search_engine  3500 non-null   bool   \n",
      " 31  Referral_Source_social_media   3500 non-null   bool   \n",
      " 32  Referral_Source_unknown        3500 non-null   bool   \n",
      "dtypes: bool(26), float64(7)\n",
      "memory usage: 307.6 KB\n",
      "\n",
      "Structure of the evaluation dataset:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_data(dataset, numerical_cols, categorical_cols, is_training=True):\n",
    "    # 1. Fill missing values for numerical variables\n",
    "    for col in numerical_cols:\n",
    "        mean_value = dataset[col].mean()\n",
    "        dataset[col] = dataset[col].fillna(mean_value)\n",
    "\n",
    "    # 2. Fill missing values for categorical variables\n",
    "    for col in categorical_cols:\n",
    "        if col in ['Payment_Method', 'Referral_Source']:\n",
    "            # Fill specific columns with 'unknown'\n",
    "            dataset[col] = dataset[col].fillna('unknown')\n",
    "        else:\n",
    "            # Fill other categorical columns with the mode\n",
    "            dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Preprocess the training dataset\n",
    "data = preprocess_data(data, numerical_cols, categorical_cols, is_training=True)\n",
    "\n",
    "# Preprocess the evaluation dataset\n",
    "eval = preprocess_data(eval, numerical_cols, categorical_cols, is_training=False)\n",
    "\n",
    "# 5. Perform one-hot encoding on categorical variables\n",
    "data = pd.get_dummies(data, columns=categorical_cols, drop_first=False)\n",
    "eval = pd.get_dummies(eval, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "# Check the structure of the datasets\n",
    "print(\"Structure of the training dataset:\\n\", data.info())\n",
    "print(\"\\nStructure of the evaluation dataset:\\n\", eval.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRBDlM9NKoI9",
    "outputId": "0a410a6f-6d95-4594-99ad-acda54f58ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Feature List: ['Age', 'Reviews_Read', 'Price', 'Discount', 'Items_In_Cart', 'Socioeconomic_Status_Score', 'Engagement_Score', 'Gender_0.0', 'Gender_1.0', 'Category_0.0', 'Category_1.0', 'Category_2.0', 'Category_3.0', 'Category_4.0', 'Time_of_Day_afternoon', 'Time_of_Day_evening', 'Time_of_Day_morning', 'Email_Interaction_0.0', 'Email_Interaction_1.0', 'Device_Type_desktop', 'Device_Type_mobile', 'Device_Type_tablet', 'Payment_Method_bank', 'Payment_Method_cash', 'Payment_Method_credit', 'Payment_Method_paypal', 'Payment_Method_unknown', 'Referral_Source_ads', 'Referral_Source_direct', 'Referral_Source_email', 'Referral_Source_search_engine', 'Referral_Source_social_media', 'Referral_Source_unknown']\n"
     ]
    }
   ],
   "source": [
    "# Print the final feature list\n",
    "final_feature_list = eval.columns.tolist()\n",
    "print(\"\\nFinal Feature List:\", final_feature_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical variables in the training dataset after standardization:\n",
      "          Age  Reviews_Read     Price  Discount  Items_In_Cart  \\\n",
      "id                                                              \n",
      "0  -1.011846     -1.176996  0.423223  0.004929       0.167870   \n",
      "1  -0.846814      1.130648  0.162018  1.762468       1.560092   \n",
      "2  -1.176879     -0.600085 -0.048892 -0.416881       0.515926   \n",
      "3  -0.599265     -0.023174  0.934084 -0.627786       0.515926   \n",
      "4  -0.846814     -0.600085 -1.626803  0.707944      -0.180185   \n",
      "\n",
      "    Socioeconomic_Status_Score  Engagement_Score  \n",
      "id                                                \n",
      "0                     1.296648         -0.051821  \n",
      "1                     1.413043          0.881906  \n",
      "2                    -1.156393         -0.104129  \n",
      "3                     0.627372          0.987935  \n",
      "4                    -0.818845          1.358504  \n",
      "\n",
      "Numerical variables in the evaluation dataset after standardization:\n",
      "            Age  Reviews_Read     Price  Discount  Items_In_Cart  \\\n",
      "id                                                                \n",
      "6500 -0.434232     -0.600085 -1.302836  0.356437      -0.528241   \n",
      "6501 -0.061937     -1.753907  0.170088 -0.416881      -1.224352   \n",
      "6502 -0.351716      1.707558 -1.676166 -0.627786       1.908148   \n",
      "6503 -0.061937     -1.753907  1.054597  0.075230       2.604259   \n",
      "6504 -1.176879      2.284469  0.002737  0.356437      -0.876296   \n",
      "\n",
      "      Socioeconomic_Status_Score  Engagement_Score  \n",
      "id                                                  \n",
      "6500                    0.164699         -1.374001  \n",
      "6501                   -0.519126         -1.484618  \n",
      "6502                    0.740858         -0.772141  \n",
      "6503                   -0.789746         -0.491735  \n",
      "6504                    0.662291          1.222090  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize numerical variables in the training dataset\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Standardize numerical variables in the evaluation dataset (using the same scaler)\n",
    "eval[numerical_cols] = scaler.transform(eval[numerical_cols])\n",
    "\n",
    "# Check the results after standardization\n",
    "print(\"Numerical variables in the training dataset after standardization:\\n\", data[numerical_cols].head())\n",
    "print(\"\\nNumerical variables in the evaluation dataset after standardization:\\n\", eval[numerical_cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lanxi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1： SMOTE without New feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CI3qmWGez5c",
    "outputId": "487330b1-fa48-4642-923c-5527972d1f86"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define the feature variables X and the target variable y\n",
    "X = data[final_feature_list]  # Retain only the final selected feature variables\n",
    "y = data['Purchase']  # Extract the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after resampling:\n",
      "Purchase\n",
      "0    5744\n",
      "1    5744\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Resampled feature data dimensions: (11488, 33)\n",
      "\n",
      "Resampled target variable dimensions: (11488,)\n"
     ]
    }
   ],
   "source": [
    "# Case 1 - Adjust data imbalance using SMOTE (oversampling)\n",
    "# Define SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Perform oversampling\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the distribution of classes after resampling\n",
    "print(\"Class distribution after resampling:\")\n",
    "print(y_resampled.value_counts())\n",
    "\n",
    "# Check the structure of the resampled data\n",
    "print(\"\\nResampled feature data dimensions:\", X_resampled.shape)\n",
    "print(\"\\nResampled target variable dimensions:\", y_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training set: 9190\n",
      "Number of samples in the testing set: 2298\n",
      "Class distribution in the training set:\n",
      "Purchase\n",
      "1    4616\n",
      "0    4574\n",
      "Name: count, dtype: int64\n",
      "Class distribution in the testing set:\n",
      "Purchase\n",
      "0    1170\n",
      "1    1128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "# Train with oversampled data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the split results\n",
    "print(f\"Number of samples in the training set: {X_train.shape[0]}\")\n",
    "print(f\"Number of samples in the testing set: {X_test.shape[0]}\")\n",
    "print(f\"Class distribution in the training set:\\n{y_train.value_counts()}\")\n",
    "print(f\"Class distribution in the testing set:\\n{y_test.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated F1-Score (Before Hyperparameter Tuning): 0.9258 ± 0.0036\n",
      "\n",
      "Initial Model Evaluation:\n",
      "F1-score: 0.9229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1145   25]\n",
      " [ 140  988]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93      1170\n",
      "           1       0.98      0.88      0.92      1128\n",
      "\n",
      "    accuracy                           0.93      2298\n",
      "   macro avg       0.93      0.93      0.93      2298\n",
      "weighted avg       0.93      0.93      0.93      2298\n",
      "\n",
      "ROC-AUC Score: 0.9662\n",
      "\n",
      "Starting Grid Search for Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "Best Parameters from Grid Search:\n",
      "{'C': 10, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Retraining model with the best hyperparameters...\n",
      "\n",
      "Optimized F1-score: 0.9245\n",
      "\n",
      "Optimized Confusion Matrix:\n",
      "[[1151   19]\n",
      " [ 142  986]]\n",
      "\n",
      "Optimized Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93      1170\n",
      "           1       0.98      0.87      0.92      1128\n",
      "\n",
      "    accuracy                           0.93      2298\n",
      "   macro avg       0.94      0.93      0.93      2298\n",
      "weighted avg       0.93      0.93      0.93      2298\n",
      "\n",
      "Cross-Validated F1-Score (After Hyperparameter Tuning): 0.9277 ± 0.0041\n",
      "Optimized ROC-AUC Score: 0.9662\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Define the initial Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Evaluate the initial model's F1-score using 5-fold cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Cross-Validated F1-Score (Before Hyperparameter Tuning): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Train the model with initial parameters\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics for the initial model\n",
    "print(\"\\nInitial Model Evaluation:\")\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate the ROC-AUC score\n",
    "y_pred_proba = log_reg.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Hyperparameter tuning: Grid Search with Cross-Validation\n",
    "print(\"\\nStarting Grid Search for Hyperparameter Tuning...\")\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Optimization algorithms\n",
    "    'penalty': ['l2'],  # Use L2 regularization\n",
    "    'class_weight': ['balanced', None]  # Handle imbalanced data\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000),\n",
    "    param_grid,\n",
    "    scoring='f1',  # Optimize for F1-score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1  # Display detailed search progress\n",
    ")\n",
    "\n",
    "# Perform grid search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"\\nBest Parameters from Grid Search:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Retrain the model using the best parameters\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "print(\"\\nRetraining model with the best hyperparameters...\")\n",
    "best_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_best_pred = best_log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the optimized F1-score\n",
    "best_f1 = f1_score(y_test, y_best_pred)\n",
    "print(f\"\\nOptimized F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# Print the optimized confusion matrix and classification report\n",
    "print(\"\\nOptimized Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_best_pred))\n",
    "\n",
    "print(\"\\nOptimized Classification Report:\")\n",
    "print(classification_report(y_test, y_best_pred))\n",
    "\n",
    "# Cross-validation evaluation for the optimized model\n",
    "best_cv_scores = cross_val_score(best_log_reg, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Cross-Validated F1-Score (After Hyperparameter Tuning): {best_cv_scores.mean():.4f} ± {best_cv_scores.std():.4f}\")\n",
    "\n",
    "# Calculate the ROC-AUC score for the optimized model\n",
    "y_best_pred_proba = best_log_reg.predict_proba(X_test)[:, 1]\n",
    "optimized_roc_auc = roc_auc_score(y_test, y_best_pred_proba)\n",
    "print(f\"Optimized ROC-AUC Score: {optimized_roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction file has been generated with columns: id, Purchase\n",
      "     id  Purchase\n",
      "0  6500         0\n",
      "1  6501         0\n",
      "2  6502         0\n",
      "3  6503         0\n",
      "4  6504         0\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "# Step 1: Ensure the eval dataset contains the same features as the training data\n",
    "eval_processed = eval[final_feature_list]\n",
    "\n",
    "# Step 2: Make predictions using the trained model\n",
    "eval_predictions = best_log_reg.predict(eval_processed)\n",
    "\n",
    "# Step 3: Create a DataFrame for the prediction results\n",
    "eval_results = pd.DataFrame({\n",
    "    'id': eval.index,  # Assuming `id` is the index of the eval dataset\n",
    "    'Purchase': eval_predictions  # Predicted classification results\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "eval_results.to_csv(r'D:\\kaggle\\result\\logistical_case1.csv', index=False)\n",
    "\n",
    "print(\"Prediction file has been generated with columns: id, Purchase\")\n",
    "print(eval_results.head())  # View the first few rows of the generated prediction results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2： NO SMOTE without New feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated F1-Score (Before Hyperparameter Tuning): 0.2322 ± 0.0191\n",
      "\n",
      "Initial Model Evaluation:\n",
      "F1-score: 0.2000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1129   14]\n",
      " [ 138   19]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94      1143\n",
      "           1       0.58      0.12      0.20       157\n",
      "\n",
      "    accuracy                           0.88      1300\n",
      "   macro avg       0.73      0.55      0.57      1300\n",
      "weighted avg       0.85      0.88      0.85      1300\n",
      "\n",
      "\n",
      "Starting Grid Search for Hyperparameter Tuning...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "Best Parameters from Grid Search:\n",
      "{'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Retraining model with the best hyperparameters...\n",
      "\n",
      "Optimized F1-score: 0.3537\n",
      "\n",
      "Optimized Confusion Matrix:\n",
      "[[816 327]\n",
      " [ 53 104]]\n",
      "\n",
      "Optimized Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.71      0.81      1143\n",
      "           1       0.24      0.66      0.35       157\n",
      "\n",
      "    accuracy                           0.71      1300\n",
      "   macro avg       0.59      0.69      0.58      1300\n",
      "weighted avg       0.85      0.71      0.76      1300\n",
      "\n",
      "Cross-Validated F1-Score (After Hyperparameter Tuning): 0.3829 ± 0.0163\n"
     ]
    }
   ],
   "source": [
    "# Case 2 - Without SMOTE, using Logistic Regression's built-in balancing\n",
    "# Define the feature variables X and the target variable y\n",
    "X = data[final_feature_list]  # Retain only the final selected feature variables\n",
    "y = data['Purchase']  # Extract the target variable\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the initial Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Evaluate the initial model's F1-score using 5-fold cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Cross-Validated F1-Score (Before Hyperparameter Tuning): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Train the model with initial parameters\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics for the initial model\n",
    "print(\"\\nInitial Model Evaluation:\")\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Hyperparameter tuning: Grid Search with Cross-Validation\n",
    "print(\"\\nStarting Grid Search for Hyperparameter Tuning...\")\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Optimization algorithms\n",
    "    'penalty': ['l2'],  # Use L2 regularization\n",
    "    'class_weight': ['balanced', None]  # Handle imbalanced data\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000),\n",
    "    param_grid,\n",
    "    scoring='f1',  # Optimize for F1-score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1  # Display detailed search progress\n",
    ")\n",
    "\n",
    "# Perform grid search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters\n",
    "print(\"\\nBest Parameters from Grid Search:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Retrain the model using the best parameters\n",
    "best_log_reg = grid_search.best_estimator_\n",
    "print(\"\\nRetraining model with the best hyperparameters...\")\n",
    "best_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_best_pred = best_log_reg.predict(X_test)\n",
    "\n",
    "# Calculate the optimized F1-score\n",
    "best_f1 = f1_score(y_test, y_best_pred)\n",
    "print(f\"\\nOptimized F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# Print the optimized confusion matrix and classification report\n",
    "print(\"\\nOptimized Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_best_pred))\n",
    "\n",
    "print(\"\\nOptimized Classification Report:\")\n",
    "print(classification_report(y_test, y_best_pred))\n",
    "\n",
    "# Cross-validation evaluation for the optimized model\n",
    "best_cv_scores = cross_val_score(best_log_reg, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Cross-Validated F1-Score (After Hyperparameter Tuning): {best_cv_scores.mean():.4f} ± {best_cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction file has been generated with columns: id, Purchase\n",
      "     id  Purchase\n",
      "0  6500         0\n",
      "1  6501         0\n",
      "2  6502         1\n",
      "3  6503         0\n",
      "4  6504         0\n"
     ]
    }
   ],
   "source": [
    "# Case 2\n",
    "# Step 1: Ensure the eval dataset contains the same features as the training data\n",
    "eval_processed = eval[final_feature_list]\n",
    "\n",
    "# Step 2: Make predictions using the trained model\n",
    "eval_predictions = best_log_reg.predict(eval_processed)\n",
    "\n",
    "# Step 3: Create a DataFrame for the prediction results\n",
    "eval_results = pd.DataFrame({\n",
    "    'id': eval.index,  # Assuming `id` is the index of the eval dataset\n",
    "    'Purchase': eval_predictions  # Predicted classification results\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "eval_results.to_csv(r'D:\\kaggle\\result\\logistical_case2.csv', index=False)\n",
    "\n",
    "print(\"Prediction file has been generated with columns: id, Purchase\")\n",
    "print(eval_results.head())  # View the first few rows of the generated prediction results\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
